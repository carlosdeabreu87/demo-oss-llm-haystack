{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# FOR GOOGLE COLAB\\n\\n!git clone https://github.com/carlosdeabreu87/demo-oss-llm-haystack.git\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# FOR GOOGLE COLAB\n",
    "\n",
    "!git clone https://github.com/carlosdeabreu87/demo-oss-llm-haystack.git\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# FOR GOOGLE COLAB\\n\\nimport os\\n\\n# Change the current working directory to \\'project\\'\\nos.chdir(\\'/content/demo-oss-llm-haystack\\')\\n\\n# Verify the current working directory\\nprint(\"Current Working Directory: \", os.getcwd())\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# FOR GOOGLE COLAB\n",
    "\n",
    "import os\n",
    "\n",
    "# Change the current working directory to 'project'\n",
    "os.chdir('/content/demo-oss-llm-haystack')\n",
    "\n",
    "# Verify the current working directory\n",
    "print(\"Current Working Directory: \", os.getcwd())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# FOR GOOGLE COLAB\\n\\n!pip install -r ./requirements.txt\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# FOR GOOGLE COLAB\n",
    "\n",
    "!pip install -r ./requirements.txt\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# FOR GOOGLE COLAB\\n\\n!huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GGUF mistral-7b-instruct-v0.1.Q4_K_M.gguf --local-dir ./models --local-dir-use-symlinks False\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# FOR GOOGLE COLAB\n",
    "\n",
    "!huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GGUF mistral-7b-instruct-v0.1.Q4_K_M.gguf --local-dir ./models --local-dir-use-symlinks False\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\req-test-3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\ProgramData\\Anaconda3\\envs\\req-test-3\\Lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\ProgramData\\Anaconda3\\envs\\req-test-3\\Lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\ProgramData\\Anaconda3\\envs\\req-test-3\\Lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\ProgramData\\Anaconda3\\envs\\req-test-3\\Lib\\site-packages\\quantulum3\\classifier.py:12: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n",
      "c:\\ProgramData\\Anaconda3\\envs\\req-test-3\\Lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\ProgramData\\Anaconda3\\envs\\req-test-3\\Lib\\site-packages\\PyPDF2\\__init__.py:21: DeprecationWarning: PyPDF2 is deprecated. Please move to the pypdf library instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "from haystack.document_stores import InMemoryDocumentStore, WeaviateDocumentStore\n",
    "from haystack.nodes import (\n",
    "    EmbeddingRetriever\n",
    "    ,PreProcessor\n",
    "    ,TextConverter\n",
    "    ,PromptNode\n",
    "    ,PromptTemplate\n",
    "    ,TopPSampler\n",
    "    ,PromptModel\n",
    "    )\n",
    "from haystack.nodes.ranker import LostInTheMiddleRanker\n",
    "from haystack.pipelines import Pipeline\n",
    "from pathlib import Path\n",
    "from llm_model_config import LlamaCPPInvocationLayer\n",
    "from pdf_to_txt_converter import pdf_to_txt_converter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#warnings.filterwarnings(\"ignore\", message=\"*DeprecationWarning*\")\n",
    "#warnings.filterwarnings(\"ignore\", message=\"*UserWarning*\")\n",
    "#warnings.filterwarnings(\"ignore\", message=\"*Tqdm*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Auxiliary function for converting pdf files contents to PDF\n",
    "\n",
    "# pdf_to_txt_converter(pdf_file_path = 'data/se_best_practices.pdf', txt_file_path = 'data/se_best_practices.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from models/mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
      "...............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 32768\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  4096.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 4096.00 MiB, K (f16): 2048.00 MiB, V (f16): 2048.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    36.50 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =  1040.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n"
     ]
    }
   ],
   "source": [
    "## Definition of the nodes for the pipeline\n",
    "\n",
    "document_store = InMemoryDocumentStore(embedding_dim = 384)\n",
    "\n",
    "converter = TextConverter()\n",
    "\n",
    "preprocessor = PreProcessor(\n",
    "                            clean_empty_lines = True\n",
    "                           ,clean_whitespace = False\n",
    "                           ,clean_header_footer = True\n",
    "                           ,split_by = \"word\"\n",
    "                           ,split_length = 250\n",
    "                           ,split_respect_sentence_boundary = True\n",
    "                           )\n",
    "\n",
    "embedding_retriever = EmbeddingRetriever(\n",
    "                                          document_store = document_store \n",
    "                                         ,embedding_model = \"sentence-transformers/All-MiniLM-L6-V2\" \n",
    "                                         ,model_format = \"sentence_transformers\" \n",
    "                                         ,top_k=10\n",
    "                                        )\n",
    "\n",
    "llm_model = PromptModel(\n",
    "                        model_name_or_path = \"models/mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\n",
    "                       ,invocation_layer_class = LlamaCPPInvocationLayer\n",
    "                       ,use_gpu = True\n",
    "                       ,max_length = 512\n",
    "                       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 37.04docs/s]\n",
      "Batches: 100%|██████████| 1/1 [00:10<00:00, 10.21s/it]ocs/s]\n",
      "Documents Processed: 10000 docs [00:10, 977.51 docs/s]       \n"
     ]
    }
   ],
   "source": [
    "## Function for building the indexing the pipeline which stores the embedded documents in the vector store\n",
    "\n",
    "def indexing_pipeline(local_dir):\n",
    "\n",
    "    # Iterate over files in the directory\n",
    "    for filename in os.listdir(local_dir):\n",
    "        file_path = Path(local_dir) / filename\n",
    "        \n",
    "        # Check if the file is a text file\n",
    "        if file_path.suffix == '.txt':\n",
    "            documents = converter.convert(file_path = str(file_path), meta=None)\n",
    "\n",
    "    preprocessed_docs = preprocessor.process(documents)\n",
    "\n",
    "    document_store.write_documents(preprocessed_docs)\n",
    "\n",
    "    document_store.update_embeddings(embedding_retriever)\n",
    "\n",
    "\n",
    "indexing_pipeline(local_dir = \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for building the query the pipeline which takes the user prompts, enrich its, query the vector store and passes it down to the LLM for the final response\n",
    "\n",
    "def query_pipeline(query):\n",
    "    \n",
    "    prompt_text = \"\"\"\n",
    "Synthesize a comprehensive answer from the provided paragraphs of a \n",
    "paper and the given question.\\n\n",
    "Focus on the question and avoid unnecessary information in your answer.\\n\n",
    "\\n\\n Paragraphs: {join(documents)} \\n\\n Question: {query} \\n\\n Answer:\n",
    "\"\"\"\n",
    "\n",
    "    prompt_node = PromptNode(\n",
    "                             model_name_or_path = llm_model\n",
    "                            ,default_prompt_template = PromptTemplate(prompt_text)\n",
    "                            ,max_length = 384\n",
    "                            ,model_kwargs = {\"stream\": False}\n",
    "                            )   \n",
    "\n",
    "    \n",
    "    query_pipeline = Pipeline()\n",
    "\n",
    "    query_pipeline.add_node(\n",
    "                            component = embedding_retriever\n",
    "                           ,name = \"Retriever\"\n",
    "                           ,inputs = [\"Query\"]\n",
    "                           )\n",
    "    \n",
    "    query_pipeline.add_node(\n",
    "                            component = TopPSampler(top_p = 0.90)\n",
    "                           ,name = \"Sampler\"\n",
    "                           ,inputs = [\"Retriever\"]\n",
    "                           )\n",
    "    \n",
    "    query_pipeline.add_node(\n",
    "                            component = LostInTheMiddleRanker(1024)\n",
    "                           ,name = \"LostInTheMiddleRanker\"\n",
    "                           ,inputs = [\"Sampler\"]\n",
    "                           )\n",
    "    \n",
    "    query_pipeline.add_node(\n",
    "                            component = prompt_node\n",
    "                           ,name = \"Prompt\"\n",
    "                           ,inputs = [\"LostInTheMiddleRanker\"]\n",
    "                           )\n",
    "\n",
    "    pipeline_obj = query_pipeline.run(query = query)\n",
    "    \n",
    "    return pipeline_obj[\"results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 39.08it/s]\n",
      "\n",
      "llama_print_timings:        load time =  131951.40 ms\n",
      "llama_print_timings:      sample time =      11.16 ms /    16 runs   (    0.70 ms per token,  1433.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =  224847.32 ms /   459 tokens (  489.86 ms per token,     2.04 tokens per second)\n",
      "llama_print_timings:        eval time =   11933.76 ms /    15 runs   (  795.58 ms per token,     1.26 tokens per second)\n",
      "llama_print_timings:       total time =  236971.76 ms /   474 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This document is about software development methodologies and how they can be improved to be'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Example 1\n",
    "\n",
    "response = query_pipeline(query = \"What is this document about?\")\n",
    "response[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 47.62it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  131951.40 ms\n",
      "llama_print_timings:      sample time =      10.63 ms /    16 runs   (    0.66 ms per token,  1505.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =   49779.97 ms /   105 tokens (  474.09 ms per token,     2.11 tokens per second)\n",
      "llama_print_timings:        eval time =   12066.40 ms /    15 runs   (  804.43 ms per token,     1.24 tokens per second)\n",
      "llama_print_timings:       total time =   62027.62 ms /   120 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n Carlos De Abreu is not the author of this draft. The paragraph'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Example 2\n",
    "\n",
    "response = query_pipeline(query = \"Is Carlos De Abreu the author of this draft?\")\n",
    "response[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.58it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  131951.40 ms\n",
      "llama_print_timings:      sample time =      12.57 ms /    16 runs   (    0.79 ms per token,  1272.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =  211004.42 ms /   413 tokens (  510.91 ms per token,     1.96 tokens per second)\n",
      "llama_print_timings:        eval time =   13397.31 ms /    15 runs   (  893.15 ms per token,     1.12 tokens per second)\n",
      "llama_print_timings:       total time =  224597.85 ms /   428 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The authors of this draft are Daniel Huttenlocher and Daniel Spoon'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Example 3\n",
    "\n",
    "response = query_pipeline(query = \"who the authors of this draft?\")\n",
    "response[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Interface to submit user prompts to the LLM using the query pipeline\n",
    "\n",
    "def ask_llm(prompt):\n",
    "    answer = query_pipeline(query = prompt)\n",
    "    answer = answer[0]\n",
    "    return answer\n",
    "\n",
    "demo = gr.Interface(fn=ask_llm, inputs=\"textbox\", outputs=\"textbox\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(inbrowser=True)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "haystack-tutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
